<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DeepSeaNet Portal</title>
  <link rel="icon" href="https://editor.analyticsvidhya.com/uploads/59360487px-OpenCV_Logo_with_text_svg_version.svg.png" type="image/x-icon">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700&display=swap">
  <style>
    :root {
      --primary-bg-color: #f0f0f0; /* Light mode background color */
      --primary-text-color: #333;  /* Light mode text color */
      --secondary-bg-color: #4285F4; /* Light mode secondary background color */
    }

    /* Dark mode CSS variables */
    :root[data-theme='dark'] {
      --primary-bg-color: #333; /* Dark mode background color */
      --primary-text-color: #f0f0f0; /* Dark mode text color */
      --secondary-bg-color: #1a73e8; /* Dark mode secondary background color */
    }

    body {
      font-family: 'Roboto', sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background-color: var(--primary-bg-color);
      color: var(--primary-text-color);
    }

    header {
      background-color: var(--secondary-bg-color);
      color: #fff;
      padding: 10px;
      text-align: center;
    }

    .container {
      max-width: 900px;
      margin: 20px auto;
      padding: 20px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
      border-radius: 5px;
      background-color: #fff; /* Light mode container background color */
    }

    /* Dark mode container background color */
    :root[data-theme='dark'] .container {
      background-color: #444;
    }

    .image-section {
      text-align: center;
      margin-bottom: 20px;
    }

    .image-section img {
      max-width: 100%;
      border-radius: 5px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .results-section,
    .summary-section,
    .references-section {
      margin-bottom: 20px;
    }

    .reference {
      margin-bottom: 10px;
    }

    .smooth-button {
      background-color: var(--secondary-bg-color);
      color: #fff;
      border: none;
      padding: 10px 20px;
      border-radius: 5px;
      cursor: pointer;
      transition: background-color 0.3s;
    }

    .smooth-button:hover {
      background-color: #357AE8;
    }

    /* Switch button for mode toggle */
    .mode-switch {
      position: absolute;
      top: 10px;
      right: 10px;
      background-color: var(--secondary-bg-color);
      color: #fff;
      border: none;
      padding: 4px;
      border-radius: 20px;
      cursor: pointer;
      display: flex;
      align-items: center;
      font-size: 12px;
      letter-spacing: 1px;
    }

    .mode-switch input {
      visibility: hidden;
      position: absolute;
    }

    .mode-switch .slider {
      position: relative;
      width: 30px;
      height: 16px;
      background-color: #ccc;
      border-radius: 8px;
      margin: 0 8px;
      transition: background-color 0.3s;
    }

    .mode-switch .slider:before {
      content: "";
      position: absolute;
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background-color: #fff;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      left: 2px;
      top: 2px;
      transition: transform 0.3s;
    }

    input:checked + .slider {
      background-color: var(--secondary-bg-color);
    }

    input:checked + .slider:before {
      transform: translateX(14px);
    }
  </style>
</head>
<body>
  <header>
    <h1>DeepSeaNet Project Portal</h1>
  </header>

  <!-- Switch button for mode toggle -->
  <label class="mode-switch">
    <input type="checkbox" id="modeToggle">
    <div class="slider"></div>
    <span id="modeText">Night Mode</span>
  </label>

  <div class="container">
    <div class="image-section">
      <img src="https://raw.githubusercontent.com/s4nyam/deepseanetportal/main/asset_files/arch.png" alt="architecture image">
    </div>

    <div class="results-section">
      <h2>Abstract</h2>
      <!-- Add your research results here -->
      <p>Marine animals and deep underwater objects are difficult to recognize and monitor for safety of aquatic life. There is an increasing challenge when the water is saline with granular particles and impurities. In such natural adversarial environment, traditional approaches like CNN start to fail and are expensive to compute. This project involves implementing and evaluating various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset, called the “Brackish-Dataset”. The dataset comprises annotated image sequences of fish, crabs, starfish, and other aquatic animals captured in Limfjorden water with limited visibility. The aim of this research project is to study the efficiency of newer models on the same dataset and contrast them with the previous results based on accuracy and inference time. Firstly, I compare the results of YOLOv3 (31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%), YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with skip connections) to perform complex feature fusion in adversarial noise which makes modified EfficientDet robust to perturbations. Third, analyzed the effect on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning (98.04% mAP). Last, I provide classactivation-map based explanations (CAM) for the two models to promote Explainability in black box models. Overall, the results indicate that modified EfficientDet achieved higher accuracy with five-fold cross validation than the other models with 88.54% IoU of feature maps.</p>
    </div>

    <div class="summary-section">
      <h2>Contributions</h2>
      <!-- Add your research summary here -->
      <h2>Contributions and state-of-the-art object detection models with techniques:</h2>
      <ol>
          <li>
              <h3>Comparative evaluation of state-of-the-art object detection models:</h3>
              <p>The project involves a comparative evaluation of various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset. This evaluation helps to identify the most effective model for underwater object detection.</p>
          </li>
          <li>
              <h3>Development of a modified BiSkFPN mechanism:</h3>
              <p>The project proposes a modified BiSkFPN mechanism, which involves using a BiFPN neck with skip connections, to perform complex feature fusion in adversarial noise. This mechanism makes the modified EfficientDet model more robust to perturbations and improves its accuracy.</p>
          </li>
          <li>
              <h3>Adversarial learning for improved object detection accuracy:</h3>
              <p>The project analyzes the effect of adversarial learning on the accuracy of EfficientDet and YOLOv5. This analysis helps to identify the most effective training strategy for underwater object detection.</p>
          </li>
          <li>
              <h3>Explainability in black box models:</h3>
              <p>The project provides class-activation-map based explanations (CAM) for EfficientDet and YOLOv5 to promote explainability in black box models. These explanations help to improve the interpretability of the models and enhance their trustworthiness.</p>
          </li>
          <li>
              <h3>Improved performance in AMODMV:</h3>
              <p>The project demonstrates that modified EfficientDet achieved higher accuracy with five-fold cross-validation than other models. The use of EfficientDet improves the accuracy and reliability of AMODMV, which can enhance maritime security, monitor the environment, and protect aquatic life.</p>
          </li>
      </ol>
      <h2>Requirements and Configuration used</h2>
      <img src="./asset_files/systemConf.png" width="100%">
    </div>

    <div class="references-section">
      <h2>Important Figures from the paper</h2>
      
      <p>A modified Bidirectional Skip-Connection FPN (BiSkFPN) bottleneck is proposed by modifying original BiFPN layer. BiSkFPN architecture is shown in Figure 7 which has an additional layer of Deconv feature maps and skip connections. On contrary to the previous research, that has used attention mechanism to preserve low-level features, I use Deconv feature maps which are less computationally expensive than attention mechanism and robust to perturbations.</p>
      <img src="./asset_files/model.png" width="100%">
    <p>A feature map is the output of a convolutional layer that highlights specific features in the input data. Each feature map contains a set of values that represent the presence or absence of a specific feature in the input. The purpose of the feature map is to extract high-level features from the input data that are relevant to the task at hand. Attention mechanisms are used to selectively focus on certain parts of the input data that are most relevant to the task at hand. An attention layer can be added between two convolutional blocks to learn feature weights based on the input data. Deconvolution or transposed convolution layers can be used to up-sample the feature maps and increase their spatial resolution. A deconvolution layer can be added between two convolutional blocks to recover lost spatial information.</p>
    
    <img src="./asset_files/training.png" width="100%">
    <p>Figure 9 shows train-test accuracy curves of different models used in the experiment to compare. The train and test accuracy curve is a plot of the accuracy of a model on the training and testing datasets over epochs, as the model is trained. Furthermore, by utilizing UAP adversarial noise as part of the adversarial learning approach in the training procedure of EfficientDet for the dataset, the proposed approach demonstrates significant improvements in both training and testing accuracy and loss, also depicted in Figure 9. UAP was utilized in object detection training procedure by adding small perturbations to the input image.</p>
    <img src="./asset_files/adv.png" width="100%">
<p>In addition, such noise is imperceptible to time-limited humans [30]. Therefor I provide an example in Figure 10 where the same ground truth image was tested for proposed EfficientDet model before and after UAP attack, along with the predictions from proposed EfficientDet with Adversarial Learning (AL).</p>
Finally, the GradCAM++ tool (introduced in section 4.2.5) is utilized to understand the class-specific features for the proposed EfficientDet model and YOLOv8 shown in Figure 11. It is particularly useful for understanding how the model makes its decision, and which parts of the image are most important for that decision. Visualizing class-specific features using GradCAM++ is important because it can help us understand why a model is making a certain prediction, and whether it is focusing on the correct features. By visualizing the regions of an image that are most relevant for a specific class, we can gain insights into the model's decision-making process and potentially improve its accuracy.
<img src="./asset_files/fig.png" width="100%">


    <div class="smooth-button">
      <a href="https://arxiv.org/pdf/2306.06075.pdf" target="_blank" style="color: white; text-decoration: none;">Download Full Paper - https://arxiv.org/abs/2306.06075</a>
  </div>
  <br/>
  <div class="smooth-button">
    <a href="https://github.com/s4nyam/efficientdet-advml" target="_blank" style="color: white; text-decoration: none;">GitHub Repository - https://github.com/s4nyam/efficientdet-advml</a>
</div>
  </div>

  <script>
    // JavaScript function to set the mode to Night Mode
    function setNightMode() {
      document.documentElement.setAttribute('data-theme', 'dark');
      document.getElementById('modeText').textContent = 'Night Mode';
    }

    // JavaScript function to set the mode to Light Mode
    function setLightMode() {
      document.documentElement.setAttribute('data-theme', 'light');
      document.getElementById('modeText').textContent = 'Light Mode';
    }

    // Listen for changes in the checkbox state to toggle between Night Mode and Light Mode
    document.getElementById('modeToggle').addEventListener('change', function() {
      if (this.checked) {
        setNightMode();
      } else {
        setLightMode();
      }
    });

    // Initialize the theme based on the user's preference
    const userPreference = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    document.documentElement.setAttribute('data-theme', userPreference);
    document.getElementById('modeToggle').checked = userPreference === 'dark';
    document.getElementById('modeText').textContent = userPreference === 'dark' ? 'Night Mode' : 'Light Mode';
  </script>
</body>
</html>
